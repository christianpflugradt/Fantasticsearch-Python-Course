# 2B - filter and tokenize text

Difficulty: 2/5

Improve upon the ```tokenize``` function.

The implemented ```tokenize``` function returns a list of strings containing parts of html code.
The function should be improved to do the following:
* remove all html tags from these strings
* remove all special characters from these strings
* convert all letters to lowercase
* split each string into words
* remove duplicate tokens

Use the following regular expressions to remove html tags and special characters:
```
tagsub = r'<.+?>'
specialsub = r'[^a-zA-Z0-9 \n\.]'
```

You should use the ```re``` module to apply regular expressions.

String: ```'<h2 id="test">This is a <b>string</b>. This string looks <i>nice</i>!</h2>'```

Should be converted to: ```['this', 'is', 'a', 'string', 'looks', 'nice']```

## Interface ##

File: [tokenize.py](workspace/tokenize.py)

Function name: ```tokenize```

Parameters: html content of the JEP as string

Return value: collection of unique tokens as strings

## Tips ##

<details>
  <summary>show tips</summary>

* try to import ```re``` in your REPL, then inspect it with ```dir()```
* ```sub(match, replace, text)``` will replace all matches for ```match``` in ```text``` with ```replace```
* ```text.split(char)``` will split the string ```text``` by all appearances of ```char```: 
  ```'a,b,c'.split(',') == ['a', 'b', 'c']```
* lists can contain duplicates while sets cannot; a list converted to a set will have all duplicate entries removed
* You cannot concat two sets but you can use the union operator ```|=``` to add elements from one set to another:
    ```a = {1,2,3}; a |= {1,2,4}; print(a)``` will print ```{1,2,3,4}```
</details>

## Solution ##

<details>
  <summary>show solution</summary>

```
from re import findall, sub

textmatch = r'<h2 id=.+?>.+?</h2>(.+?)<h2>'
finaltextmatch = r'<h2 id=.+?>.+?</h2>(.+?)</div>'
tagsub = r'<.+?>'
specialsub = r'[^a-zA-Z0-9 \n\.]'

def find_texts(content):
    return findall(textmatch, content) + findall(finaltextmatch, content)
    
def remove_tags(text):
    return sub(tagsub, ' ', text)
    
def remove_symbols(text):
    return sub(specialsub, ' ', text)

def split_tokens(text):
    return text.split(' ')

def tokenize(content):
    tokens = set()
    for text in find_texts(content):
        tokens |= set(split_tokens(remove_symbols(remove_tags(text)).lower()))
    return tokens
```
</details>

## Navigation ##
* [back to overview](0.md)
