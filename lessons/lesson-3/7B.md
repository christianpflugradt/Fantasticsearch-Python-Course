# 7B - improve tokenizing speed

Difficulty: 2/5

You must have at least finished sub lessons 6A and 7A before starting this one,
but it's recommended to have finished all previous sub lessons beforehand.

Improve upon the `load_data` function. Start a new thread for every JEP being tokenized.
Limit the maximum number of threads tokenizing JEPs to 4. You may also set a higher limit
and observe the impact on tokenizing performance.

You should use the `threading` module to start threads.

You should use the `concurrency` module written in sub lesson 7A where necessary.

## Interface ##

File: [dataloader.py](workspace/load_data.py)

Function name: `load_data`

Parameters: *unchanged*

Return value: *unchanged*

## Tips ##

You learnt everything you need to know in the previous sub lessons.

## Solution ##

<details>
  <summary>show solution</summary>

```
from concurrency import get_number_of_threads as count, wait_until_threshold_reached as wait_for
from crawler import crawl
from database import exists, restore, save
from threading import Thread
from tokenizer import tokenize

DB_FILE = 'fantasticdb.db'

jepdb = []

def add_to_jepdb(id, text, content):
    print('tokenizing jep %s' % id)
    jepdb.append((id, text, tokenize(content)))

def process(jeps):
    threshold = count()
    for id, text, content in jeps:
        wait_for(threshold + 4)
        Thread(target=add_to_jepdb, args=(id, text, content)).start()
    wait_for(threshold)

def load_data(callback):
    global jepdb
    if exists(DB_FILE):
        print('restoring jeps...')
        jepdb = restore(DB_FILE)
    else:
        print('downloading jeps...')
        jeps = crawl()
        print('tokenizing jeps...')
        process(jeps)
        save(jepdb, DB_FILE)
    callback(jepdb)
```
</details>

## Navigation ##
* [back to overview](0.md)
